{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cEZ1JNa-qQT",
        "outputId": "f96350d4-d72b-44c9-d0ee-f0cb3558db0e"
      },
      "outputs": [],
      "source": [
        "!pip install monai==0.7.0\n",
        "\n",
        "!pip install monai tqdm\n",
        "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "!pip install self-attention-cv==1.2.3\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from monai.apps import DecathlonDataset\n",
        "from monai.config import print_config\n",
        "from monai.data import DataLoader\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    Activations,\n",
        "    AsChannelFirstd,\n",
        "    AsDiscrete,\n",
        "    CenterSpatialCropd,\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    MapTransform,\n",
        "    NormalizeIntensityd,\n",
        "    Orientationd,\n",
        "    RandFlipd,\n",
        "    RandScaleIntensityd,\n",
        "    RandShiftIntensityd,\n",
        "    RandSpatialCropd,\n",
        "    Spacingd,\n",
        "    ToTensord,\n",
        ")\n",
        "from monai.utils import set_determinism\n",
        "\n",
        "import torch\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQYksPFz_dmw",
        "outputId": "d86ae518-984d-4660-9cd7-8fde9947d4de"
      },
      "outputs": [],
      "source": [
        "root_dir = './'\n",
        "print(root_dir)\n",
        "set_determinism(seed=0)\n",
        "\n",
        "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
        "    \"\"\"\n",
        "    Convert labels to multi channels based on brats classes:\n",
        "    label 1 is the peritumoral edema\n",
        "    label 2 is the GD-enhancing tumor\n",
        "    label 3 is the necrotic and non-enhancing tumor core\n",
        "    The possible classes are TC (Tumor core), WT (Whole tumor)\n",
        "    and ET (Enhancing tumor).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, data):\n",
        "        d = dict(data)\n",
        "        for key in self.keys:\n",
        "            result = []\n",
        "            # merge label 2 and label 3 to construct TC\n",
        "            result.append(np.logical_or(d[key] == 2, d[key] == 3))\n",
        "            # merge labels 1, 2 and 3 to construct WT\n",
        "            result.append(\n",
        "                np.logical_or(\n",
        "                    np.logical_or(d[key] == 2, d[key] == 3), d[key] == 1\n",
        "                )\n",
        "            )\n",
        "            # label 2 is ET\n",
        "            result.append(d[key] == 2)\n",
        "            d[key] = np.stack(result, axis=0).astype(np.float32)\n",
        "        return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qdhrybh_jLo"
      },
      "outputs": [],
      "source": [
        "roi_size=[128, 128, 64]\n",
        "pixdim=(1.5, 1.5, 2.0)\n",
        "\n",
        "train_transform = Compose(\n",
        "    [\n",
        "        # load 4 Nifti images and stack them together\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        AsChannelFirstd(keys=\"image\"),\n",
        "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=pixdim,\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        RandSpatialCropd(\n",
        "            keys=[\"image\", \"label\"], roi_size=roi_size, random_size=False),\n",
        "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
        "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=0.5),\n",
        "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=0.5),\n",
        "        ToTensord(keys=[\"image\", \"label\"]),\n",
        "    ]\n",
        ")\n",
        "val_transform = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        AsChannelFirstd(keys=\"image\"),\n",
        "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=pixdim,\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        CenterSpatialCropd(keys=[\"image\", \"label\"], roi_size=roi_size),\n",
        "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "        ToTensord(keys=[\"image\", \"label\"]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQid9Tn5_k5g",
        "outputId": "a1316387-4d3a-4944-b72f-dd3b8cf1696e"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
        "cache_num = 8\n",
        "\n",
        "train_ds = DecathlonDataset(\n",
        "    root_dir=root_dir,\n",
        "    task=\"Task01_BrainTumour\",\n",
        "    transform=train_transform,\n",
        "    section=\"training\",\n",
        "    download=True,\n",
        "    num_workers=4,\n",
        "    cache_num=cache_num, # it was 100 but we use larger volumes\n",
        "    cache_rate=0.01\n",
        ")\n",
        "\n",
        "val_ds = DecathlonDataset(\n",
        "    root_dir=root_dir,\n",
        "    task=\"Task01_BrainTumour\",\n",
        "    transform=val_transform,\n",
        "    section=\"validation\",\n",
        "    download=False,\n",
        "    num_workers=4,\n",
        "    cache_num=cache_num,\n",
        "    cache_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvU3h6UZ_xcw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "# Manually filter 0.01 of the data\n",
        "train_size = int(len(train_ds) * 1)\n",
        "val_size = int(len(val_ds) * 1)\n",
        "\n",
        "train_indices = np.random.choice(len(train_ds), train_size, replace=False)\n",
        "val_indices = np.random.choice(len(val_ds), val_size, replace=False)\n",
        "\n",
        "train_subset = torch.utils.data.Subset(train_ds, train_indices)\n",
        "val_subset = torch.utils.data.Subset(val_ds, val_indices)\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=2, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_subset, batch_size=2, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "XbwkhTEfCJg4",
        "outputId": "b00fe9d8-8653-4f7e-a189-56efc95a0698"
      },
      "outputs": [],
      "source": [
        "slice_id = 32\n",
        "# pick one image from DecathlonDataset to visualize and check the 4 channels\n",
        "print(f\"image shape: {val_ds[2]['image'].shape}\")\n",
        "plt.figure(\"image\", (24, 6))\n",
        "for i in range(4):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    plt.title(f\"image channel {i}\")\n",
        "    plt.imshow(val_ds[2][\"image\"][i, :, :, slice_id].detach().cpu(),  cmap=\"gray\") #\n",
        "plt.show()\n",
        "# also visualize the 3 channels label corresponding to this image\n",
        "print(f\"label shape: {val_ds[2]['label'].shape}\")\n",
        "plt.figure(\"label\", (24, 6))\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.title(f\"label channel {i}\")\n",
        "    plt.imshow(val_ds[6][\"label\"][i, :, :, slice_id].detach().cpu())\n",
        "plt.show()\n",
        "\n",
        "train_size = tuple(val_ds[6]['image'].shape[1:])\n",
        "print(train_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7PzJjQZMiZV"
      },
      "source": [
        "cct transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlpIxQqgMg5_"
      },
      "outputs": [],
      "source": [
        "# import torch.nn as nn\n",
        "# class MultiHeadSelfAttention(nn.Module):\n",
        "#     def __init__(self, dim, heads, dim_head):\n",
        "#         super().__init__()\n",
        "#         self.heads = heads\n",
        "#         self.dim_head = dim if dim_head is None else dim_head\n",
        "#         self.scale = self.dim_head ** -0.5\n",
        "\n",
        "#         self.to_qkv = nn.Linear(dim, self.dim_head * heads * 3, bias=False)\n",
        "#         self.to_out = nn.Linear(self.dim_head * heads, dim)\n",
        "\n",
        "#     def forward(self, x, mask=None):\n",
        "#         b, n, _, h = *x.shape, self.heads\n",
        "#         qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "#         q, k, v = map(lambda t: t.reshape(b, n, h, -1).transpose(1, 2), qkv)\n",
        "\n",
        "#         dots = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "#         if mask is not None:\n",
        "#             dots.masked_fill_(mask == 0, float('-inf'))\n",
        "#         attn = dots.softmax(dim=-1)\n",
        "\n",
        "#         out = torch.matmul(attn, v)\n",
        "#         out = out.transpose(1, 2).reshape(b, n, -1)\n",
        "#         return self.to_out(out)\n",
        "# class TransformerBlock(nn.Module):\n",
        "\n",
        "#     def __init__(self, dim, heads=8, dim_head=None,\n",
        "#                  dim_linear_block=1024, dropout=0.1, activation=nn.GELU,\n",
        "#                  mhsa=None, prenorm=False):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             dim: token's vector length\n",
        "#             heads: number of heads\n",
        "#             dim_head: if none dim/heads is used\n",
        "#             dim_linear_block: the inner projection dim\n",
        "#             dropout: probability of droppping values\n",
        "#             mhsa: if provided you can change the vanilla self-attention block\n",
        "#             prenorm: if the layer norm will be applied before the mhsa or after\n",
        "#         \"\"\"\n",
        "#         super().__init__()\n",
        "#         self.mhsa = mhsa if mhsa is not None else MultiHeadSelfAttention(dim=dim, heads=heads, dim_head=dim_head)\n",
        "#         self.prenorm = prenorm\n",
        "#         self.drop = nn.Dropout(dropout)\n",
        "#         self.norm_1 = nn.LayerNorm(dim)\n",
        "#         self.norm_2 = nn.LayerNorm(dim)\n",
        "\n",
        "#         self.linear = nn.Sequential(\n",
        "#             nn.Linear(dim, dim_linear_block),\n",
        "#             activation(),  # nn.ReLU or nn.GELU\n",
        "#             nn.Dropout(dropout),\n",
        "#             nn.Linear(dim_linear_block, dim),\n",
        "#             nn.Dropout(dropout)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x, mask=None):\n",
        "#         if self.prenorm:\n",
        "#             y = self.drop(self.mhsa(self.norm_1(x), mask)) + x\n",
        "#             out = self.linear(self.norm_2(y)) + y\n",
        "#         else:\n",
        "#             y = self.norm_1(self.drop(self.mhsa(x, mask)) + x)\n",
        "#             out = self.norm_2(self.linear(y) + y)\n",
        "#         return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuOjR5M4gtV9"
      },
      "source": [
        "tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyfMB5gAgs28"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from einops import rearrange\n",
        "\n",
        "# class MultiHeadSelfAttention(nn.Module):\n",
        "#     def __init__(self, dim, heads, dim_head=None, scale=None):\n",
        "#         super().__init__()\n",
        "#         self.heads = heads\n",
        "#         self.dim_head = (dim // heads) if dim_head is None else dim_head\n",
        "#         self.scale = scale if scale is not None else (self.dim_head ** -0.5)\n",
        "\n",
        "#         self.to_qkv = nn.Linear(dim, self.dim_head * heads * 3, bias=False)\n",
        "#         self.to_out = nn.Linear(self.dim_head * heads, dim)\n",
        "\n",
        "#     def forward(self, x, mask=None):\n",
        "#         b, n, _ = x.shape\n",
        "#         qkv = self.to_qkv(x)\n",
        "#         qkv = rearrange(qkv, 'b n (h d qkv) -> qkv b h n d', h=self.heads, qkv=3)\n",
        "#         q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "#         dots = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "#         if mask is not None:\n",
        "#             dots = dots.masked_fill(mask == 0, float('-inf'))\n",
        "#         attn = dots.softmax(dim=-1)\n",
        "\n",
        "#         out = torch.matmul(attn, v)\n",
        "#         out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "#         return self.to_out(out)\n",
        "\n",
        "# class TransformerBlock1(nn.Module):\n",
        "#     def __init__(self, dim, heads=8, dim_head=None, dim_linear_block=1024, dropout=0.1, activation=nn.GELU, prenorm=False):\n",
        "#         super().__init__()\n",
        "#         self.prenorm = prenorm\n",
        "#         self.mhsa = MultiHeadSelfAttention(dim, heads, dim_head)\n",
        "#         self.norm1 = nn.LayerNorm(dim)\n",
        "#         self.norm2 = nn.LayerNorm(dim)\n",
        "#         self.drop = nn.Dropout(dropout)\n",
        "#         self.activation = activation()\n",
        "#         self.feed_forward = nn.Sequential(\n",
        "#             nn.Linear(dim, dim_linear_block),\n",
        "#             self.activation,\n",
        "#             nn.Dropout(dropout),\n",
        "#             nn.Linear(dim_linear_block, dim),\n",
        "#             nn.Dropout(dropout)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x, mask=None):\n",
        "#         if self.prenorm:\n",
        "#             x = self.norm1(x)\n",
        "#             mhsa_out = self.mhsa(x, mask)\n",
        "#             x = x + self.drop(mhsa_out)\n",
        "#             x = self.norm2(x)\n",
        "#             ff_out = self.feed_forward(x)\n",
        "#             return x + ff_out\n",
        "#         else:\n",
        "#             mhsa_out = self.mhsa(x, mask)\n",
        "#             x = self.norm1(x + self.drop(mhsa_out))\n",
        "#             ff_out = self.feed_forward(x)\n",
        "#             return self.norm2(x + ff_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9RzRmMbEZ5D"
      },
      "source": [
        "new transformer with relative pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwe2T4BwEGF5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from einops import rearrange\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, heads, dim_head=None, scale=None, relative_pos_embedding=False):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.dim_head = (dim // heads) if dim_head is None else dim_head\n",
        "        self.scale = scale if scale is not None else (self.dim_head ** -0.5)\n",
        "        self.relative_pos_embedding = relative_pos_embedding\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, self.dim_head * heads * 3, bias=False)\n",
        "        self.to_out = nn.Linear(self.dim_head * heads, dim)\n",
        "\n",
        "        if self.relative_pos_embedding:\n",
        "            self.relative_pos_embedding = nn.Embedding(2 * 512, self.dim_head)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        b, n, _ = x.shape\n",
        "        qkv = self.to_qkv(x)\n",
        "        qkv = rearrange(qkv, 'b n (h d qkv) -> qkv b h n d', h=self.heads, qkv=3)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if self.relative_pos_embedding:\n",
        "            relative_pos_ids = self._get_relative_pos_ids(n)\n",
        "            relative_pos_embeddings = self.relative_pos_embedding(relative_pos_ids)\n",
        "            dots += relative_pos_embeddings\n",
        "\n",
        "        if mask is not None:\n",
        "            dots = dots.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "    def _get_relative_pos_ids(self, n):\n",
        "        relative_pos_ids = torch.arange(n).unsqueeze(0).repeat(n, 1)\n",
        "        relative_pos_ids = relative_pos_ids - torch.arange(n).unsqueeze(1)\n",
        "        relative_pos_ids = relative_pos_ids.unsqueeze(0)\n",
        "        relative_pos_ids = relative_pos_ids.to(self.relative_pos_embedding.weight.device)\n",
        "        return relative_pos_ids\n",
        "\n",
        "class TransformerBlock1(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=None, dim_linear_block=1024, dropout=0.1, activation=nn.GELU, prenorm=False, cross_layer_norm=False, relative_pos_embedding=False):\n",
        "        super().__init__()\n",
        "        self.prenorm = prenorm\n",
        "        self.cross_layer_norm = cross_layer_norm\n",
        "        self.mhsa = MultiHeadSelfAttention(dim, heads, dim_head, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.activation = activation()\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(dim, dim_linear_block),\n",
        "            self.activation,\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_linear_block, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        if self.cross_layer_norm:\n",
        "            self.norm3 = nn.LayerNorm(dim)\n",
        "            self.norm4 = nn.LayerNorm(dim)\n",
        "\n",
        "        self.pos_encoding = PositionalEncoding(dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        if self.prenorm:\n",
        "            x = self.norm1(x)\n",
        "            mhsa_out = self.mhsa(x, mask)\n",
        "            if self.cross_layer_norm:\n",
        "                mhsa_out = self.norm3(mhsa_out)\n",
        "            x = x + self.drop(mhsa_out)\n",
        "\n",
        "            x = self.norm2(x)\n",
        "            ff_out = self.feed_forward(x)\n",
        "            if self.cross_layer_norm:\n",
        "                ff_out = self.norm4(ff_out)\n",
        "            x = x + self.drop(ff_out)\n",
        "        else:\n",
        "            mhsa_out = self.mhsa(x, mask)\n",
        "            x = self.norm1(x + self.drop(mhsa_out))\n",
        "\n",
        "            ff_out = self.feed_forward(x)\n",
        "            x = self.norm2(x + self.drop(ff_out))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "new transformer with dssa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from einops import rearrange\n",
        "\n",
        "# Positional Encoding class as originally provided\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# DSSA class adapted for use with sequence data\n",
        "class DSSA(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=None, scale=None, window_size=7):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.dim_head = (dim // heads) if dim_head is None else dim_head\n",
        "        self.scale = scale if scale is not None else (self.dim_head ** -0.5)\n",
        "        self.window_size = window_size\n",
        "        inner_dim = self.dim_head * heads\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _ = x.shape\n",
        "        qkv = self.to_qkv(x)\n",
        "        qkv = rearrange(qkv, 'b n (h d qkv) -> qkv b h n d', h=self.heads, qkv=3)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        q *= self.scale\n",
        "        dots = torch.matmul(q, k.transpose(-2, -1))\n",
        "        attn = dots.softmax(dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "# Modified TransformerBlock to use DSSA\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=None, dim_linear_block=1024, dropout=0.1, activation=nn.GELU, prenorm=False, cross_layer_norm=False, window_size=7):\n",
        "        super().__init__()\n",
        "        self.prenorm = prenorm\n",
        "        self.cross_layer_norm = cross_layer_norm\n",
        "        self.dssa = DSSA(dim, heads, dim_head, window_size=window_size)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.activation = activation()\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(dim, dim_linear_block),\n",
        "            self.activation,\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_linear_block, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        if self.cross_layer_norm:\n",
        "            self.norm3 = nn.LayerNorm(dim)\n",
        "            self.norm4 = nn.LayerNorm(dim)\n",
        "        self.pos_encoding = PositionalEncoding(dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.pos_encoding(x)\n",
        "        if self.prenorm:\n",
        "            x = self.norm1(x)\n",
        "            dssa_out = self.dssa(x)\n",
        "            if self.cross_layer_norm:\n",
        "                dssa_out = self.norm3(dssa_out)\n",
        "            x = x + self.drop(dssa_out)\n",
        "            x = self.norm2(x)\n",
        "            ff_out = self.feed_forward(x)\n",
        "            if self.cross_layer_norm:\n",
        "                ff_out = self.norm4(ff_out)\n",
        "            x = x + self.drop(ff_out)\n",
        "        else:\n",
        "            dssa_out = self.dssa(x)\n",
        "            x = self.norm1(x + self.drop(dssa_out))\n",
        "            ff_out = self.feed_forward(x)\n",
        "            x = self.norm2(x + self.drop(ff_out))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7oZTStGMpF9"
      },
      "source": [
        "TranspConv3DBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp477QHZOKUv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TranspConv3DBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1,\n",
        "                 norm_type='batch', activation='relu', use_dropout=False, dropout_prob=0.3):\n",
        "        super(TranspConv3DBlock, self).__init__()\n",
        "\n",
        "        # Transpose convolution layer\n",
        "        self.transp_conv = nn.ConvTranspose3d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size,\n",
        "            stride=stride, padding=padding, output_padding=output_padding, bias=False)\n",
        "\n",
        "        # Normalization layer\n",
        "        if norm_type == 'batch':\n",
        "            self.norm = nn.BatchNorm3d(out_channels)\n",
        "        elif norm_type == 'instance':\n",
        "            self.norm = nn.InstanceNorm3d(out_channels)\n",
        "        else:\n",
        "            self.norm = None\n",
        "\n",
        "        # Activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU(inplace=True)\n",
        "        elif activation == 'leaky_relu':\n",
        "            self.activation = nn.LeakyReLU(0.01, inplace=True)\n",
        "        else:\n",
        "            self.activation = None\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout3d(dropout_prob) if use_dropout else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.transp_conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TranspConv3DBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21GmFwZuP5rf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TranspConv3DBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1, norm_layer=None):\n",
        "        super(TranspConv3DBlock, self).__init__()\n",
        "        # Transpose convolution layer\n",
        "        self.transp_conv = nn.ConvTranspose3d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size,\n",
        "            stride=stride, padding=padding, output_padding=output_padding, bias=False)\n",
        "\n",
        "        # Normalization layer, if provided (e.g., BatchNorm3d or InstanceNorm3d)\n",
        "        self.norm = norm_layer(out_channels) if norm_layer else None\n",
        "\n",
        "        # Activation function\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.transp_conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WLup0BjTqem"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Embeddings3D(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, cube_size, patch_size, dropout):\n",
        "        super(Embeddings3D, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Compute the number of patches along each dimension\n",
        "        self.num_patches_x = cube_size[0] // patch_size\n",
        "        self.num_patches_y = cube_size[1] // patch_size\n",
        "        self.num_patches_z = cube_size[2] // patch_size\n",
        "        self.num_patches = self.num_patches_x * self.num_patches_y * self.num_patches_z\n",
        "\n",
        "        # Convolutional layer for patch embedding\n",
        "        self.proj = nn.Conv3d(input_dim, embed_dim, kernel_size=patch_size, stride=patch_size, bias=False)\n",
        "\n",
        "        # Positional embeddings with dynamic sizing\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, embed_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, channels, depth, height, width)\n",
        "        # Transform input to (batch_size, embed_dim, num_patches)\n",
        "        x = self.proj(x)  # Output shape: (batch_size, embed_dim, D', H', W')\n",
        "\n",
        "        # Flatten the patch dimensions\n",
        "        x = x.flatten(2)  # Flatten D', H', W' into a single dimension\n",
        "        x = x.transpose(1, 2)  # Change to (batch_size, num_patches, embed_dim)\n",
        "\n",
        "        # Add positional embedding and apply dropout\n",
        "        embeddings = x + self.pos_embed\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        # Apply normalization\n",
        "        embeddings = self.norm(embeddings)\n",
        "\n",
        "        return embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "newUNETR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p45WNQhVMsE9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "\n",
        "from self_attention_cv.UnetTr.modules import  BlueBlock\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_layers, dropout, extract_layers, dim_linear_block):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.extract_layers = extract_layers\n",
        "\n",
        "        # makes TransformerBlock device aware\n",
        "        self.block_list = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.block_list.append(TransformerBlock1(dim=embed_dim, heads=num_heads,\n",
        "                                                    dim_linear_block=dim_linear_block, dropout=dropout, prenorm=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        extract_layers = []\n",
        "        for depth, layer_block in enumerate(self.block_list):\n",
        "            x = layer_block(x)\n",
        "            if (depth + 1) in self.extract_layers:\n",
        "                extract_layers.append(x)\n",
        "\n",
        "        return extract_layers\n",
        "\n",
        "class newUNETR(nn.Module):\n",
        "    def __init__(self, img_shape=(128, 128, 128), input_dim=4, output_dim=3,\n",
        "                 embed_dim=768, patch_size=16, num_heads=12, dropout=0.0,\n",
        "                 ext_layers=[3, 6, 9, 12], norm='instance',\n",
        "                 base_filters=16,\n",
        "                 dim_linear_block=3072):\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_layers = 12\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.img_shape = img_shape\n",
        "        self.patch_size = patch_size\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.ext_layers = ext_layers\n",
        "        self.patch_dim = [int(x / patch_size) for x in img_shape]\n",
        "\n",
        "        self.norm = nn.BatchNorm3d if norm == 'batch' else nn.InstanceNorm3d\n",
        "\n",
        "        self.embed = Embeddings3D(input_dim=input_dim, embed_dim=embed_dim,\n",
        "                                  cube_size=img_shape, patch_size=patch_size, dropout=dropout)\n",
        "\n",
        "        self.transformer = TransformerEncoder(embed_dim, num_heads,\n",
        "                                              self.num_layers, dropout, ext_layers,\n",
        "                                              dim_linear_block=dim_linear_block)\n",
        "\n",
        "        self.init_conv = Conv3DBlock(input_dim, base_filters, double=True, norm=self.norm)\n",
        "\n",
        "        # blue blocks in Fig.1\n",
        "        self.z3_blue_conv = BlueBlock(in_planes=embed_dim, out_planes=base_filters * 2, layers=3)\n",
        "\n",
        "        self.z6_blue_conv = BlueBlock(in_planes=embed_dim, out_planes=base_filters * 4, layers=2)\n",
        "\n",
        "        self.z9_blue_conv = BlueBlock(in_planes=embed_dim, out_planes=base_filters * 8, layers=1)\n",
        "\n",
        "        # Green blocks in Fig.1\n",
        "        self.z12_deconv = TranspConv3DBlock(embed_dim, base_filters * 8)\n",
        "\n",
        "        self.z9_deconv = TranspConv3DBlock(base_filters * 8, base_filters * 4)\n",
        "        self.z6_deconv = TranspConv3DBlock(base_filters * 4, base_filters * 2)\n",
        "        self.z3_deconv = TranspConv3DBlock(base_filters * 2, base_filters)\n",
        "\n",
        "        # Yellow blocks in Fig.1\n",
        "        self.z9_conv = Conv3DBlock(base_filters * 8 * 2, base_filters * 8, double=True, norm=self.norm)\n",
        "        self.z6_conv = Conv3DBlock(base_filters * 4 * 2, base_filters * 4, double=True, norm=self.norm)\n",
        "        self.z3_conv = Conv3DBlock(base_filters * 2 * 2, base_filters * 2, double=True, norm=self.norm)\n",
        "        # out convolutions\n",
        "        self.out_conv = nn.Sequential(\n",
        "            # last yellow conv block\n",
        "            Conv3DBlock(base_filters * 2, base_filters, double=True, norm=self.norm),\n",
        "            # grey block, final classification layer\n",
        "            nn.Conv3d(base_filters, output_dim, kernel_size=1, stride=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        transf_input = self.embed(x)\n",
        "        z3, z6, z9, z12 = map(lambda t: rearrange(t, 'b (x y z) d -> b d x y z',\n",
        "                                                  x=self.patch_dim[0], y=self.patch_dim[1], z=self.patch_dim[2]),\n",
        "                              self.transformer(transf_input))\n",
        "\n",
        "        # Blue convs\n",
        "        z0 = self.init_conv(x)\n",
        "        z3 = self.z3_blue_conv(z3)\n",
        "        z6 = self.z6_blue_conv(z6)\n",
        "        z9 = self.z9_blue_conv(z9)\n",
        "\n",
        "        # Green block for z12\n",
        "        z12 = self.z12_deconv(z12)\n",
        "        # Concat + yellow conv\n",
        "        y = torch.cat([z12, z9], dim=1)\n",
        "        y = self.z9_conv(y)\n",
        "\n",
        "        # Green block for z6\n",
        "        y = self.z9_deconv(y)\n",
        "        # Concat + yellow conv\n",
        "        y = torch.cat([y, z6], dim=1)\n",
        "        y = self.z6_conv(y)\n",
        "\n",
        "        # Green block for z3\n",
        "        y = self.z6_deconv(y)\n",
        "        # Concat + yellow conv\n",
        "        y = torch.cat([y, z3], dim=1)\n",
        "        y = self.z3_conv(y)\n",
        "\n",
        "        y = self.z3_deconv(y)\n",
        "        y = torch.cat([y, z0], dim=1)\n",
        "        return self.out_conv(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7zrwB0UwgEc",
        "outputId": "68ac506a-7192-4e0a-c6c3-ec7cb5c94210"
      },
      "outputs": [],
      "source": [
        "#from monai.networks.nets import UNETR as UNETR_monai\n",
        "\n",
        "# from self_attention_cv import UNETR\n",
        "device = torch.device(\"cuda:0\")\n",
        "num_heads = 10 # 12 normally\n",
        "embed_dim= 512 # 768 normally\n",
        "\n",
        "roi_size=[128, 128, 64]\n",
        "newModel = newUNETR(img_shape=tuple(roi_size), input_dim=4, output_dim=3,\n",
        "                 embed_dim=embed_dim, patch_size=16, num_heads=num_heads,\n",
        "               ext_layers=[3, 6, 9, 12], norm='instance',\n",
        "                 base_filters=16,\n",
        "                 dim_linear_block=2048).to(device)\n",
        "\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in newModel.parameters())/1000000\n",
        "print('Parameters in millions:',pytorch_total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUEerL1TCQlD",
        "outputId": "4e07397a-ebc0-4e11-c756-96e7a5d62fa3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "\n",
        "loss_function = DiceCELoss(to_onehot_y=False, sigmoid=True)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(newModel.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "max_epochs = 50\n",
        "val_interval = 1\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "metric_values_tc = []\n",
        "metric_values_wt = []\n",
        "metric_values_et = []\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
        "    newModel.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    for batch_data in train_loader:\n",
        "        step += 1\n",
        "        inputs, labels = (\n",
        "            batch_data[\"image\"].to(device),\n",
        "            batch_data[\"label\"].to(device),\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        outputs = newModel(inputs)\n",
        "\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        newModel.eval()\n",
        "        with torch.no_grad():\n",
        "            dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "            post_trans = Compose(\n",
        "                [Activations(sigmoid=True), AsDiscrete(threshold_values=True)]\n",
        "            )\n",
        "            metric_sum = metric_sum_tc = metric_sum_wt = metric_sum_et = 0.0\n",
        "            metric_count = (\n",
        "                metric_count_tc\n",
        "            ) = metric_count_wt = metric_count_et = 0\n",
        "            for val_data in val_loader:\n",
        "                val_inputs, val_labels = (\n",
        "                    val_data[\"image\"].to(device),\n",
        "                    val_data[\"label\"].to(device),\n",
        "                )\n",
        "                val_outputs = newModel(val_inputs)\n",
        "                val_outputs = post_trans(val_outputs)\n",
        "\n",
        "                # compute overall mean dice\n",
        "                value, not_nans = dice_metric(y_pred=val_outputs, y=val_labels)\n",
        "                not_nans = not_nans.mean().item()\n",
        "                metric_count += not_nans\n",
        "                metric_sum += value.mean().item() * not_nans\n",
        "                # compute mean dice for TC\n",
        "                value_tc, not_nans = dice_metric(\n",
        "                    y_pred=val_outputs[:, 0:1], y=val_labels[:, 0:1]\n",
        "                )\n",
        "                not_nans = not_nans.item()\n",
        "                metric_count_tc += not_nans\n",
        "                metric_sum_tc += value_tc.item() * not_nans\n",
        "                # compute mean dice for WT\n",
        "                value_wt, not_nans = dice_metric(\n",
        "                    y_pred=val_outputs[:, 1:2], y=val_labels[:, 1:2]\n",
        "                )\n",
        "                not_nans = not_nans.item()\n",
        "                metric_count_wt += not_nans\n",
        "                metric_sum_wt += value_wt.item() * not_nans\n",
        "                # compute mean dice for ET\n",
        "                value_et, not_nans = dice_metric(\n",
        "                    y_pred=val_outputs[:, 2:3], y=val_labels[:, 2:3]\n",
        "                )\n",
        "                not_nans = not_nans.item()\n",
        "                metric_count_et += not_nans\n",
        "                metric_sum_et += value_et.item() * not_nans\n",
        "\n",
        "            metric = metric_sum / metric_count\n",
        "            metric_values.append(metric)\n",
        "            metric_tc = metric_sum_tc / metric_count_tc\n",
        "            metric_values_tc.append(metric_tc)\n",
        "            metric_wt = metric_sum_wt / metric_count_wt\n",
        "            metric_values_wt.append(metric_wt)\n",
        "            metric_et = metric_sum_et / metric_count_et\n",
        "            metric_values_et.append(metric_et)\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(\n",
        "                    newModel.state_dict(),\n",
        "                    os.path.join(root_dir, \"best_metric_model.pth\"),\n",
        "                )\n",
        "                print(\"saved new best metric model\")\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
        "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
        "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
        "                f\" at epoch: {best_metric_epoch}\"\n",
        "            )\n",
        "\n",
        "save_name = \"./last.pth\"\n",
        "torch.save(newModel.state_dict(),save_name)\n",
        "from google.colab import files\n",
        "# files.download(save_name)\n",
        "\n",
        "print(\n",
        "    f\"train completed, best_metric: {best_metric:.4f}\"\n",
        "    f\" at epoch: {best_metric_epoch}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "26TWfa8cEGh3",
        "outputId": "27904c8d-2784-4ba4-e86d-95fddcd48ca4"
      },
      "outputs": [],
      "source": [
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Epoch Average Loss\")\n",
        "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
        "y = epoch_loss_values\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y, color=\"red\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val Mean Dice\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
        "y = metric_values\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y, color=\"green\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(\"train\", (18, 6))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Val Mean Dice TC\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n",
        "y = metric_values_tc\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y, color=\"blue\")\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Val Mean Dice WT\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n",
        "y = metric_values_wt\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y, color=\"brown\")\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Val Mean Dice ET\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n",
        "y = metric_values_et\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y, color=\"purple\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j0PHu1AzEHdX",
        "outputId": "ea111864-b051-4cd4-ce15-6196593216cc"
      },
      "outputs": [],
      "source": [
        "\n",
        "newModel.load_state_dict(\n",
        "    torch.load(os.path.join(root_dir, \"best_metric_model.pth\"))\n",
        ")\n",
        "newModel.eval()\n",
        "with torch.no_grad():\n",
        "    # select one image to evaluate and visualize the model output\n",
        "    val_input = val_ds[6][\"image\"].unsqueeze(0).to(device)\n",
        "    val_output = newModel(val_input)\n",
        "    plt.figure(\"image\", (24, 6))\n",
        "    for i in range(4):\n",
        "        plt.subplot(1, 4, i + 1)\n",
        "        plt.title(f\"image channel {i}\")\n",
        "        plt.imshow(val_ds[6][\"image\"][i, :, :, 20].detach().cpu(), cmap=\"gray\")\n",
        "    plt.show()\n",
        "    # visualize the 3 channels label corresponding to this image\n",
        "    plt.figure(\"label\", (18, 6))\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.title(f\"label channel {i}\")\n",
        "        plt.imshow(val_ds[6][\"label\"][i, :, :, 20].detach().cpu())\n",
        "    plt.show()\n",
        "    # visualize the 3 channels model output corresponding to this image\n",
        "    plt.figure(\"output\", (18, 6))\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.title(f\"output channel {i}\")\n",
        "        out_tensor = torch.sigmoid(val_output[0, i, :, :, 20]).detach().cpu()\n",
        "        plt.imshow(out_tensor)\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "history_visible": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
